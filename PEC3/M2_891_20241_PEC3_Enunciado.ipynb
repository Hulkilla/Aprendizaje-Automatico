{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1q1mnorfHwx"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"https://www.uoc.edu/content/dam/news/images/noticies/2016/202-nova-marca-uoc.jpg\" align=\"left\" width=\"45%\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.891 · Aprendizaje automático · PEC3</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2024-1 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFTsh39afHwz"
   },
   "source": [
    "# PEC 3: Métodos supervisados\n",
    "\n",
    "En esta práctica veremos diferentes métodos supervisados y trataremos de optimizar diferentes métricas. Veremos como los diferentes modelos clasifican las observaciones y con cuales obtenemos mayor rendimiento. Después aplicaremos todo lo que hemos aprendido hasta ahora a un dataset nuevo simulando un caso práctico real.\n",
    "\n",
    "1. [Exploración de algoritmos supervisados](#eje1) \\\n",
    "    1.1. [Carga de datos](#eje10) \\\n",
    "    1.2. [Naive-Bayes](#eje11) \\\n",
    "    1.3. [Análisis Discriminante Lineal (LDA) y Análisis Discriminante Cuadrtático (QDA)](#eje12) \\\n",
    "    1.4. [K vecinos más próximos (KNN)](#eje13) \\\n",
    "    1.5. [Máquinas de soporte vectorial (SVM)](#eje14) \\\n",
    "    1.6. [Árboles de decisión](#eje15) \n",
    "2. [Implementación del caso práctico](#eje2)\\\n",
    "    2.1. [Carga de datos](#eje20) \\\n",
    "    2.2. [Análisis Exploratorio de Datos](#eje21) \\\n",
    "    2.3. [Preprocesamiento de Datos](#eje22) \\\n",
    "    2.4. [Modelización](#eje23) \n",
    "\n",
    "\n",
    "<u>Consideraciones generales</u>:\n",
    "\n",
    "- La solución planteada no puede utilizar métodos, funciones o parámetros declarados **_deprecated_** en futuras versiones, a excepción de la carga de datos cómo se indica posteriormente.\n",
    "- Esta PEC debe realizarse de forma **estrictamente individual**. Cualquier indicio de copia será penalizado con un suspenso (D) para todas las partes implicadas y la posible evaluación negativa de la asignatura de forma íntegra.\n",
    "- Es necesario que el estudiante indique **todas las fuentes** que ha utilizado para la realización de la PEC. De no ser así, se considerará que el estudiante ha cometido plagio, siendo penalizado con un suspenso (D) y la posible evaluación negativa de la asignatura de forma íntegra.\n",
    "\n",
    "<u>Formato de la entrega</u>:\n",
    "\n",
    "- Algunos ejercicios pueden suponer varios minutos de ejecución, por lo que la entrega debe hacerse en **formato notebook** y en **formato html**, donde se vea el código, los resultados y comentarios de cada ejercicio. Se puede exportar el notebook a HTML desde el menú File $\\to$ Download as $\\to$ HTML.\n",
    "- Existe un tipo de celda especial para albergar texto. Este tipo de celda os será muy útil para responder a las diferentes preguntas teóricas planteadas a lo largo de la actividad. Para cambiar el tipo de celda a este tipo, en el menú: Cell $\\to$ Cell Type $\\to$ Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSSB2tHkfHw0"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Nombre y apellidos:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bn0ZMDyKfHw0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import copy\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import kagglehub\n",
    "import umap\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torcheval.metrics.functional import binary_f1_score, binary_accuracy, binary_auroc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIkGtp_BfHw0"
   },
   "source": [
    "<a id='ej1'></a>\n",
    "# 1. Exploración de algoritmos supervisados\n",
    "\n",
    "## 1.1. Carga de datos\n",
    "\n",
    "El conjunto de datos Fashion MNIST proporcionado por Zalando consta de 70.000 imágenes con 10 clases diferentes de ropa repartidas uniformemente. No obstante, para esta práctica utilizaremos únicamente un subconjunto de 5.000 imágenes que consiste en 1.000 imágenes de 5 clases diferentes.\n",
    "\n",
    "Las imágenes tienen una resolución de 28x28 píxeles en escala de grises, por lo que se pueden representar utilizando un vector de 784 posiciones.\n",
    "\n",
    "El siguiente código cargará las 5.000 imágenes en la variable images y las correspondientes etiquetas (en forma numérica) en la variable labels. Podemos comprobar que la carga ha sido correcta obteniendo las dimensiones de estas dos variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "MPEdyuVLfHw1",
    "outputId": "34a46905-a29c-4263-9161-b2fd1efd6276"
   },
   "outputs": [],
   "source": [
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data[\"images\"]\n",
    "y = data[\"labels\"]\n",
    "n_classes = 5\n",
    "labels = [\"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\"]\n",
    "\n",
    "print(\"Vector Image Dimensions: {}\".format(X.shape))\n",
    "print(\"Vector Label Dimensions: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81-zMw2NfHw1"
   },
   "source": [
    "Con el siguiente código podemos ver un ejemplo de imagen de cada una de las clases. Para ello reajustamos el vector de 784 dimensiones que representa cada imagen en una matriz de tamaño 28x28 y la transponemos para mostrarla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, n_classes, figsize=(10,10))\n",
    "\n",
    "idxs = [np.where(y == i)[0] for i in range(n_classes)]\n",
    "\n",
    "for i in range(n_classes):\n",
    "    k = np.random.choice(idxs[i])\n",
    "    ax[i].imshow(X[k].reshape(28, 28), cmap=\"gray\")\n",
    "    ax[i].set_title(\"{}\".format(labels[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Implementación:</strong> \n",
    "\n",
    "Dividid el _dataset_ en dos subconjuntos, __*train*__ (80% de los datos) y __*test*__ (20% de los datos). Nombrad los conjuntos como: X_train, X_test, y_train, y_test. Utilizad la opción `random_state = 24`.\n",
    "    \n",
    "Podéis utilizar la implementación `train_test_split` de `sklearn`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder visualizar los resultados de cada algoritmo supervisado, reduciremos el dataset anterior a dos dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = umap.UMAP(n_components=2, random_state=42)\n",
    "model.fit(X_train)\n",
    "X_train_projection = model.transform(X_train)\n",
    "X_test_projection = model.transform(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "for i in range(n_classes):\n",
    "    ax.scatter(X_test_projection[y_test == i,0], X_test_projection[y_test == i,1], s=3, label=labels[i])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo de los ejercicios aprenderemos a ver gráficamente las fronteras de decisión que nos devuelven los diferentes modelos. Para ello utilizaremos la función definida a continuación, que sigue los siguientes pasos:\n",
    "\n",
    "Crear una meshgrid con los valores mínimo y máximo de 'x' e 'y'.\n",
    "Predecir el clasificador con los valores de la meshgrid.\n",
    "Hacer un reshape de los datos para tener el formato correspondiente.\n",
    "Una vez hecho esto, ya podemos hacer el gráfico de las fronteras de decisión y añadir los puntos reales. Así veremos las áreas que el modelo considera que son de una clase y las que considera que son de otra. Al poner encima los puntos veremos si los clasifica correctamente en el área que les corresponde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the meshgrid with the minimum and maximum values of the x and y axes\n",
    "x_min, x_max = X_test_projection[:, 0].min() - 1, X_test_projection[:, 0].max() + 1\n",
    "y_min, y_max = X_test_projection[:, 1].min() - 1, X_test_projection[:, 1].max() + 1\n",
    "\n",
    "# Define the function that will visualize the decision boundary\n",
    "def plot_decision_boundaries(model, X_test_projection, y_test):\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                         np.arange(y_min, y_max, 0.05))\n",
    "    \n",
    "    # Prediction by using all values of the meshgrid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Define the colors (one for each class)\n",
    "    cmap_light = ListedColormap(['gainsboro','lightgreen','peachpuff','lightcyan', 'pink'])\n",
    "    cmap_bold = ['grey','g','sandybrown','c','palevioletred']\n",
    "    \n",
    "    # Draw the borders\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Draw the points\n",
    "    for i in range(n_classes):\n",
    "        plt.scatter(X_test_projection[y_test == i,0], X_test_projection[y_test == i,1], \n",
    "                    s=3, label=labels[i], c=cmap_bold[i])\n",
    "    plt.legend()\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k77YKbhfHw2"
   },
   "source": [
    "<a id='ej11'></a>\n",
    "## 1.2. Gaussian Naïve Bayes (1 punto)\n",
    "\n",
    "El propósito de este primer ejercicio es comprender el funcionamiento del algoritmo Naïve-Bayes, un algoritmo peculiar que se basa en el teorema de Bayes para calcular la probabilidad de que una observación pertenezca a cada una de las clases. Este modelo asume que las características de entrada son independientes entre sí, lo que permite simplificar el cálculo de las probabilidades condicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYiQpphdfHw2"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "1. **Entrena un Modelo de Naïve-Bayes:** Utiliza el conjunto de datos de _train_ para entrenar un modelo de Naïve-Bayes. Emplea el clasificador `GaussianNB` de la biblioteca `sklearn` para este fin.\n",
    "\n",
    "2. **Calcula el _Accuracy_ del Modelo:** Una vez entrenado el modelo, calcula su precisión (_accuracy_) tanto en el conjunto de _train_ como en el de _test_. Esto te permitirá evaluar qué tan bien está funcionando tu modelo.\n",
    "\n",
    "3. **Calcula la Matriz de Confusión:** Utiliza el conjunto de _test_ para calcular la matriz de confusión del modelo. Esta matriz te ayudará a entender de mejor manera los aciertos y errores de tu clasificador.\n",
    "\n",
    "4. **Representa Gráficamente la Frontera de Decisión:** Finalmente, visualiza la frontera de decisión del modelo utilizando el conjunto de _test_. Puedes hacer esto con la ayuda de la función `plot_decision_boundary` que ya has creado previamente.\n",
    "\n",
    "Para realizar estos cálculos y visualizaciones, utiliza las funciones `accuracy_score` y `confusion_matrix` del paquete `metrics` de `sklearn`.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRhVuBpCfHw2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44uFc_xEfHw2"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EB_a4o-fHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong> \n",
    "  \n",
    "Análisis del ejercicio.\n",
    "\n",
    "   - ¿Cómo son las fronteras de decisión? ¿Tiene sentido que tengan esta forma con el algoritmo utilizado?\n",
    "   - ¿Cómo son las predicciones obtenidas sobre el conjunto de test?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAwzg5BafHw3",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I32OtF0CfHw3"
   },
   "source": [
    "<a id='ej12'></a>\n",
    "## 1.3 Análisis Discriminante Lineal (LDA) y Análisis Discriminante Cuadrático (QDA) (1 punto)\n",
    "\n",
    "Ahora, analizarás dos algoritmos que se basan en la transformación lineal de las características de entrada para maximizar la separación entre las clases. Estos modelos operan bajo la suposición de que las características siguen una distribución gaussiana. Esto te permitirá calcular las probabilidades condicionales de cada clase. Con estos cálculos, asignarás a cada observación la clase que presente la mayor probabilidad condicional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmyzyqT6fHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Sigue estos pasos con el dataset de entrenamiento (_train_):    \n",
    "    \n",
    "1. Entrena un modelo de Análisis Discriminante Lineal (LDA) utilizando el clasificador `LinearDiscriminantAnalysis` de `sklearn`.\n",
    "2. Calcula el _accuracy_ (precisión) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusión utilizando los datos de _test_.\n",
    "4. Representa gráficamente la frontera de decisión con los datos de _test_.\n",
    "\n",
    "Estas acciones te ayudarán a evaluar la eficacia del modelo LDA en tu conjunto de datos y a entender mejor cómo clasifica las observaciones.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mXPWfXgfHw3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLxfTyOvfHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWXabsd9fHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "\n",
    "1. Observa las fronteras de decisión que has generado. Reflexiona sobre su forma: ¿Se ajustan a lo que esperarías del algoritmo de Análisis Discriminante Lineal (LDA)? Considera la naturaleza lineal del algoritmo y cómo esto influye en la forma de las fronteras.\n",
    "2. Evalúa las predicciones realizadas sobre el conjunto de test. Analiza su precisión y cómo se distribuyen respecto a las fronteras de decisión. ¿Son coherentes estas predicciones con lo que observas en las fronteras de decisión?\n",
    "\n",
    "Estas reflexiones te permitirán comprender mejor la efectividad del modelo LDA y su adecuación para el conjunto de datos con el que estás trabajando.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7v6A60kfHw3",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5IEnCibfHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Realiza los siguientes pasos con el dataset de entrenamiento (_train_):\n",
    "\n",
    "1. Entrena un modelo de Análisis Discriminante Cuadrático (QDA) usando el clasificador `QuadraticDiscriminantAnalysis` de `sklearn`.\n",
    "2. Calcula el _accuracy_ (precisión) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusión utilizando los datos de _test_.\n",
    "4. Representa gráficamente la frontera de decisión con los datos de _test_.\n",
    "\n",
    "Estos pasos te ayudarán a evaluar cómo el modelo QDA se comporta con tu conjunto de datos, y a entender su capacidad para clasificar y diferenciar entre las clases.\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0VaPQNffHw3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhNPVO3CfHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDneGeZhssYx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l50NzPaTssYx"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "\n",
    "1. Examina las fronteras de decisión que has generado. Reflexiona sobre su forma: ¿Es coherente con lo que esperarías del algoritmo de Análisis Discriminante Cuadrático (QDA)? Considera cómo la naturaleza cuadrática del algoritmo podría influir en la forma de estas fronteras.\n",
    "2. Evalúa las predicciones realizadas sobre el conjunto de test. Observa su precisión y cómo se distribuyen en relación con las fronteras de decisión. ¿Son estas predicciones consistentes con las fronteras observadas?\n",
    "3. Reflexiona sobre las diferencias entre los algoritmos LDA y QDA. ¿En qué se distinguen en términos de supuestos, enfoque y resultados en tus datos?\n",
    "\n",
    "Este análisis te permitirá comprender las características y la eficacia de ambos modelos, LDA y QDA, y cómo se aplican a tu conjunto de datos.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmRdVVp1ssYx"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjEyHwXFfHw4"
   },
   "source": [
    "<a id='ej13'></a>\n",
    "## 1.4. KNN (1 punto)\n",
    "\n",
    "En este punto, vas a entender el funcionamiento del algoritmo KNN (K-Nearest-Neighbor), que se basa en la proximidad de los puntos de datos en un espacio de características. Analizarás sus ventajas y desventajas, y comprenderás cómo los parámetros que lo componen influyen en su comportamiento.\n",
    "\n",
    "KNN es un algoritmo de tipo supervisado basado en instancia. Esto significa:\n",
    "\n",
    "- Supervisado: Tu conjunto de datos de entrenamiento está etiquetado con la clase o resultado esperado.\n",
    "- Basado en instancia (_Lazy Learning_): El algoritmo no aprende explícitamente un modelo, como en la Regresión Logística o los árboles de decisión. En cambio, memoriza las instancias de entrenamiento y las utiliza como \"conocimiento\" en la fase de predicción.\n",
    "\n",
    "Para entender cómo funciona KNN, sigue estos pasos:\n",
    "\n",
    "1. Calcula la distancia entre el ítem a clasificar y los demás ítems del dataset de entrenamiento.\n",
    "2. Selecciona los \"k\" elementos más cercanos, es decir, aquellos con la menor distancia, según el tipo de distancia que utilices (euclídea, coseno, manhattan, etc).\n",
    "3. Realiza una \"votación de mayoría\" entre los k puntos seleccionados: la clase que predomine en estos puntos decidirá la clasificación final del ítem analizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TADnYVTPfHw4"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Realiza los siguientes pasos con el dataset de entrenamiento (_train_):\n",
    "\n",
    "1. Entrena un clasificador KNN con el hiperparámetro `n_neighbors=2` usando el clasificador `KNeighborsClassifier` de `sklearn`.\n",
    "2. Calcula el _accuracy_ (precisión) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusión utilizando los datos de _test_.\n",
    "4. Representa gráficamente la frontera de decisión con los datos de _test_.\n",
    "\n",
    "Si al entrenar el clasificador aparece un aviso (warning) y deseas ignorarlo, ejecuta el siguiente código antes del entrenamiento:\n",
    "\n",
    "`import warnings`\n",
    "`warnings.filterwarnings('ignore', message='^.*will change.*$', category=FutureWarning)`\"\n",
    "\n",
    "Esto te permitirá evaluar la efectividad del modelo KNN con `n_neighbors=2` en tu conjunto de datos, y entender cómo se comporta en términos de clasificación y separación de clases.    \n",
    "    \n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQ50-Q15fHw4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLjdWCM2fHw4"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF_2dKwAfHw8"
   },
   "source": [
    "En el modelo que has entrenado, has fijado el parámetro `n_neighbors` de forma arbitraria. Sin embargo, es posible que con otro valor obtengas una mejor predicción. Para encontrar el valor óptimo de los parámetros de un modelo (_hyperparameter tunning_), a menudo se utiliza una búsqueda de rejilla (_grid search_). Esto implica entrenar un modelo para cada combinación posible de hiperparámetros y evaluarlo mediante validación cruzada (_cross validation_) con 5 particiones estratificadas. Luego, seleccionarás la combinación de hiperparámetros que haya obtenido los mejores resultados.\n",
    "\n",
    "En este caso, te centrarás en optimizar un solo hiperparámetro:\n",
    "\n",
    "- 𝑘: el número de vecinos que se consideran para clasificar un nuevo ejemplo. Debes probar con todos los valores entre 1 y 20.\n",
    "\n",
    "Realiza este proceso para identificar el número óptimo de vecinos, lo que te permitirá mejorar la precisión de tus predicciones con el modelo KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UDe0lBofHw8"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Implementación:</strong>\n",
    "\n",
    "Para calcular el valor óptimo del hiperparámetro _k_ (`n_neighbors`), debes realizar una búsqueda de rejilla con validación cruzada. Este proceso te ayudará a encontrar el valor óptimo de _k_. Para cada valor, calcula su promedio y la desviación estándar. Luego, implementa un _heatmap_ para visualizar la precisión según los diferentes valores del hiperparámetro.\n",
    "\n",
    "Utiliza el módulo `GridSearchCV` de `sklearn` para calcular el mejor hiperparámetro. Para la visualización del _heatmap_, emplea la librería `Seaborn`.\n",
    "\n",
    "Estos pasos te permitirán identificar de manera efectiva y visual el valor de _k_ que maximiza la precisión de tu modelo KNN.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qb8GHGHOfHw8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9Af8JcrfHw8"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKBqIrRJfHw8"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Sigue estos pasos con el dataset de entrenamiento (_train_):\n",
    "\n",
    "1. Entrena un clasificador KNN utilizando el mejor hiperparámetro que hayas encontrado.\n",
    "2. Calcula el _accuracy_ (precisión) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusión utilizando los datos de _test_.\n",
    "4. Representa gráficamente la frontera de decisión con los datos de _test_.\n",
    "\n",
    "Este proceso te permitirá ver cómo el hiperparámetro óptimo que has identificado mejora la efectividad de tu modelo KNN en la clasificación de los datos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mK3awkbwfHw8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBI2kRH5fHw8"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teluDtnHfHw8"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "\n",
    "1. Comenta los resultados obtenidos en la búsqueda del mejor hiperparámetro. Reflexiona sobre cómo varió el rendimiento del modelo con los diferentes valores de `n_neighbors`.\n",
    "2. Analiza cómo se visualiza gráficamente el cambio del valor de `n_neighbors`. ¿Observas alguna tendencia o patrón claro? ¿Es coherente esta diferencia entre los dos gráficos al cambiar el parámetro?\n",
    "3. Examina las fronteras de decisión que has generado. ¿La forma de estas fronteras tiene sentido dado el algoritmo KNN utilizado? Piensa en cómo la elección del número de vecinos influye en la forma de la frontera.\n",
    "4. Evalúa las predicciones realizadas sobre el conjunto de test. Observa su precisión y cómo se distribuyen en relación con las fronteras de decisión. ¿Son estas predicciones consistentes con lo que observas en las fronteras de decisión?\n",
    "\n",
    "Este análisis te ayudará a comprender la eficacia del modelo KNN con diferentes configuraciones de `n_neighbors` y su impacto en la clasificación de los datos.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET55syCqfHw8",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoWjLa8LfHw9"
   },
   "source": [
    "<a id='ej14'></a>\n",
    "## 1.5. SVM (1 punto)\n",
    "\n",
    "En esta sección, vas a explorar las Máquinas de Vectores de Soporte (SVM), que se basan en el concepto del _Maximal Margin Classifier_ y el hiperplano.\n",
    "\n",
    "Un hiperplano en un espacio p-dimensional se define como un subespacio plano y afín de dimensiones p-1. En dos dimensiones, es una recta; en tres, un plano convencional. Para dimensiones mayores a tres, aunque no es intuitivo visualizarlo, el concepto se mantiene.\n",
    "\n",
    "Cuando los casos son perfectamente separables de manera lineal, surgen infinitos posibles hiperplanos. Para seleccionar el clasificador óptimo, utiliza el concepto de _maximal margin hyperplane_, el hiperplano que se encuentra más alejado de todas las observaciones de entrenamiento. Este se define calculando la distancia perpendicular mínima (margen) de las observaciones a un hiperplano. El hiperplano óptimo es aquel que maximiza este margen.\n",
    "\n",
    "En el proceso de optimización, debes tener en cuenta que solo las observaciones al margen o que lo violan (vectores soporte) influyen en el hiperplano. Estos vectores soporte son los que definen el clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMxzknM5fHw9"
   },
   "source": [
    "#### Los _kernels_ en SVM\n",
    "\n",
    "En situaciones donde no puedes encontrar un hiperplano que separe dos clases, es decir, cuando las clases no son linealmente separables, puedes utilizar el truco del núcleo (_kernel trick_). Este método te permite trabajar en una dimensión nueva donde es posible encontrar un hiperplano para separar las clases.\n",
    "\n",
    "Al igual que con el KNN, las SVM también dependen de varios hiperparámetros. En este caso, te enfocarás en optimizar dos hiperparámetros:\n",
    "\n",
    "1. **C**: la regularización, que es el valor de penalización de los errores en la clasificación. Este valor indica el compromiso entre obtener el hiperplano con el margen más grande posible y clasificar correctamente el máximo número de ejemplos. Debes probar los siguientes valores: 0.01, 0.1, 1, 10, 50, 100 y 200.\n",
    "   \n",
    "2. **Gamma**: un coeficiente que multiplica la distancia entre dos puntos en el kernel radial. En términos simples, cuanto más pequeño sea gamma, más influencia tendrán dos puntos cercanos. Debes probar los valores: 0.001, 0.01, 0.1, 1 y 10.\n",
    "\n",
    "Para validar el rendimiento del algoritmo con cada combinación de hiperparámetros, utiliza la validación cruzada (_cross-validation_) con 4 particiones estratificadas.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdHJe2affHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Implementación:</strong>\n",
    "\n",
    "\n",
    "1. Calcula el valor óptimo de los hiperparámetros _C_ y _gamma_ utilizando una búsqueda de rejilla con validación cruzada. Este proceso te ayudará a encontrar los valores óptimos.\n",
    "2. Para cada combinación de valores, calcula su promedio y la desviación estándar.\n",
    "3. Haz un _heatmap_ para visualizar la precisión según los diferentes valores de los hiperparámetros.\n",
    "\n",
    "Utiliza el módulo `GridSearchCV` de `sklearn` para calcular los mejores hiperparámetros con el clasificador SVC (de `SVM` de `sklearn`). Para la visualización del _heatmap_, emplea la librería `Seaborn`.\n",
    "\n",
    "Estos pasos te permitirán identificar de manera efectiva y visual los valores de _C_ y _gamma_ que maximizan la precisión de tu modelo SVM.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBxQod7LfHw9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em56OnxofHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E-ae6x0fHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Realiza los siguientes pasos con el dataset de entrenamiento (_train_):\n",
    "\n",
    "1. Entrena un modelo SVM utilizando la mejor combinación de parámetros que hayas encontrado.\n",
    "2. Calcula el _accuracy_ (precisión) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusión utilizando los datos de _test_.\n",
    "4. Representa gráficamente la frontera de decisión con los datos de _test_.\n",
    "\n",
    "Este proceso te permitirá ver cómo la mejor combinación de parámetros mejora la efectividad de tu modelo SVM en la clasificación de los datos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf2kQHuMfHw9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maHfaJpAfHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT7FOpk9fHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "\n",
    "1. Comenta los resultados obtenidos en la búsqueda de los mejores hiperparámetros. Reflexiona sobre cómo varió el rendimiento del modelo SVM con los diferentes valores de _C_ y _gamma_. Considera si los valores óptimos encontrados tienen sentido en el contexto de tu conjunto de datos.\n",
    "2. Examina las fronteras de decisión que has generado con el modelo SVM. ¿La forma de estas fronteras es coherente con lo que esperarías del algoritmo utilizado? Piensa en cómo la combinación de hiperparámetros seleccionados podría influir en la forma de las fronteras.\n",
    "3. Evalúa las predicciones realizadas sobre el conjunto de test. Observa su precisión y cómo se distribuyen en relación con las fronteras de decisión. ¿Son estas predicciones consistentes con lo que observas en las fronteras de decisión?\n",
    "\n",
    "Este análisis te ayudará a comprender la eficacia del modelo SVM con los hiperparámetros seleccionados y su impacto en la clasificación de los datos.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kan9R0kpfHw9",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9Z1MzFbfHw9"
   },
   "source": [
    "<a id='ej15'></a>\n",
    "## 1.6. Árboles de decisión (1 punto)\n",
    "\n",
    "En esta sección, vas a explorar los árboles de decisión, modelos predictivos que se basan en reglas binarias (si/no) para clasificar las observaciones según sus atributos y predecir el valor de la variable respuesta. Estos árboles pueden ser clasificadores, como en tu ejemplo, o regresores para predecir variables continuas.\n",
    "\n",
    "#### Construcción de un Árbol\n",
    "\n",
    "Para construir un árbol, sigue el algoritmo de *recursive binary splitting*:\n",
    "\n",
    "1. Comienza en la parte superior del árbol, donde todas las observaciones pertenecen a la misma región.\n",
    "2. Identifica todos los posibles puntos de corte para cada uno de los predictores. Estos puntos de corte son los diferentes niveles de los predictores.\n",
    "3. Evalúa las posibles divisiones para cada predictor utilizando una medida específica. En los clasificadores, estas medidas pueden ser el *classification error rate*, el índice Gini, la entropía o el chi-square.\n",
    "\n",
    "Comprender estos pasos te ayudará a entender cómo los árboles de decisión crean divisiones binarias para clasificar los datos y cómo estos pueden aplicarse tanto para clasificación como para regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTnIbWTYfHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Sigue estos pasos:\n",
    "\n",
    "1. Con el dataset de entrenamiento (_train_), entrena un árbol de decisión utilizando el clasificador `DecisionTreeClassifier` de la biblioteca `tree` de `sklearn`.\n",
    "2. Calcula el _accuracy_ (precisión) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusión utilizando los datos de _test_.\n",
    "4. Representa gráficamente la frontera de decisión con los datos de _test_.\n",
    "5. Representa el árbol de decisión. Puedes utilizar el comando `plot.tree` de la biblioteca `tree` de `sklearn`.\n",
    "\n",
    "Estos pasos te permitirán evaluar cómo el árbol de decisión se comporta en tu conjunto de datos, tanto en términos de clasificación como en su representación visual.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqqvClsEfHw-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpPKYCZjfHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI_q3bCafHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "\n",
    "1. Evalúa y comenta los resultados obtenidos con el árbol de decisión. Considera tanto el _accuracy_ del modelo en los conjuntos de _train_ y _test_ como los resultados de la matriz de confusión.\n",
    "2. Reflexiona sobre cómo la frontera de decisión visualizada en el conjunto de _test_ se alinea con los resultados obtenidos. ¿Es coherente con lo que esperarías de un árbol de decisión?\n",
    "3. Observa la representación gráfica del árbol. Analiza cómo las diferentes ramificaciones y decisiones tomadas en el árbol explican el comportamiento del modelo y su impacto en la clasificación de los datos.\n",
    "\n",
    "Este análisis te ayudará a comprender en profundidad el funcionamiento y la eficacia del árbol de decisión en tu conjunto de datos específico.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MABFtjLJfHw-",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlU7Tg-1fHw-"
   },
   "source": [
    "#### Evitando el *overfitting*\n",
    "\n",
    "El proceso de construcción de árboles descrito tiende a reducir rápidamente el error de entrenamiento, por lo que generalmente el modelo se ajusta muy bien a las observaciones utilizadas como entrenamiento (conjunto de *train*). Como consecuencia, los árboles de decisión tienden al *overfitting*.\n",
    "   \n",
    "Para evitar el *overfitting* en los árboles de decisión, es crucial que modifiques ciertos hiperparámetros del modelo de la siguiente manera:\n",
    "\n",
    "1. Utiliza el hiperparámetro `max_depth`, que define la profundidad máxima del árbol. Deberás explorar los valores entre 4 y 10 para encontrar el equilibrio adecuado entre la complejidad del modelo y su capacidad para generalizar.\n",
    "2. Establece el hiperparámetro `min_samples_split`, que es el número mínimo de observaciones que debe tener una hoja del árbol antes de considerar una división. Experimenta con valores como 2, 10, 20, 50 y 100 para asegurarte de que el árbol no se vuelva demasiado específico para las observaciones de entrenamiento.\n",
    "\n",
    "Ajustando estos hiperparámetros, podrás controlar la tendencia del árbol de decisión a sobreajustarse al conjunto de entrenamiento, mejorando así su capacidad para realizar predicciones efectivas en nuevos datos.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2KF6CI2fHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Implementación:</strong>\n",
    "\n",
    "1. Calcula el valor óptimo de los hiperparámetros `max_depth` y `min_samples_split` utilizando una búsqueda de rejilla con validación cruzada. Este proceso te ayudará a encontrar los valores óptimos que evitarán el sobreajuste.\n",
    "2. Para cada combinación de valores, calcula su promedio y la desviación estándar.\n",
    "3. Haz un _heatmap_ para visualizar la precisión según los diferentes valores de los hiperparámetros.\n",
    "\n",
    "Utiliza el módulo `GridSearchCV` de `sklearn` para calcular los mejores hiperparámetros con el clasificador `DecisionTreeClassifier` de `tree` de `sklearn`. Para la visualización del _heatmap_, emplea la librería `Seaborn`.\n",
    "\n",
    "Estos pasos te permitirán identificar de manera efectiva y visual los valores de `max_depth` y `min_samples_split` que maximizan la precisión de tu árbol de decisión, minimizando el riesgo de sobreajuste.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAaYYYTbfHw-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcaZzrK-fHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT9ttV8GfHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "\n",
    "1. Entrena un árbol de decisión con el dataset de entrenamiento (_train_) utilizando la mejor combinación de parámetros que hayas encontrado.\n",
    "2. Calcula el _accuracy_ (precisión) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusión utilizando los datos de _test_.\n",
    "4. Representa gráficamente la frontera de decisión con los datos de _test_.\n",
    "5. Representa el árbol de decisión.\n",
    "\n",
    "Estos pasos te permitirán evaluar cómo el árbol de decisión, ajustado con los hiperparámetros óptimos, se comporta en tu conjunto de datos, tanto en términos de clasificación como en su representación visual.\"\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3ZJ7I-FfHw-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRxJyG5MfHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cQKFNpffHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "\n",
    "1. Evalúa y comenta los resultados obtenidos en la búsqueda de los mejores hiperparámetros. Considera cómo la combinación óptima de `max_depth` y `min_samples_split` ha impactado el rendimiento del árbol de decisión.\n",
    "2. Examina las fronteras de decisión generadas con el conjunto de _test_. Reflexiona sobre si la forma de estas fronteras es coherente con lo que esperarías de un árbol de decisión configurado con estos hiperparámetros.\n",
    "3. Analiza las predicciones realizadas sobre el conjunto de test. Observa su precisión y cómo se distribuyen en relación con las fronteras de decisión. ¿Son consistentes estas predicciones con la estructura del árbol de decisión y las fronteras observadas?\n",
    "\n",
    "Este análisis te ayudará a comprender la eficacia del árbol de decisión con los hiperparámetros seleccionados y su impacto en la clasificación de los datos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3vL6gWjfHw_",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N8TJl-1fHw_"
   },
   "source": [
    "<a id='eje2'></a>\n",
    "# 2. Implementación del caso práctico (5 puntos)\n",
    "\n",
    "Hoy en día, la logística de la última milla es un problema abordado en la industria por muchas empresas dedicadas al comercio electrónico. La información proporcionada al usuario a la hora de realizar un pedido puede suponer un valor diferencial. Por ello, muchas empresas dedican muchos recursos para dar una estimación precisa sobre el tiempo que va a tardar en llegar cada pedido. En este ejercicio nos vamos a centrar en predecir el nivel de servicio de las operaciones logísticas de última milla de Amazon. En concreto identificaremos aquellas entregas que se consideren premium (tiempo de reparto inferior a dos horas).\n",
    "\n",
    "Para ello, vamos a utilizar el conjunto de datos de entregas [amazon-delivery-dataset](https://www.kaggle.com/datasets/sujalsuthar/amazon-delivery-dataset), el cual incluye datos sobre más de 43.632 entregas en varias ciudades, con información relevante sobre los detalles del pedido, los agentes de entrega, las condiciones meteorológicas y del tráfico, y las métricas de rendimiento de la entrega. En concreto, el dataset contiene 16 características:\n",
    "\n",
    "- Order_ID: identificador único de pedido\n",
    "- Agent_Age: edad del agente (repartidor)\n",
    "- Agent_Rating: puntuación del agente (repartidor)\n",
    "- Store_Latitude: latitud del almacén o tienda\n",
    "- Store_Longitude: longitud del almacén o tienda\n",
    "- Drop_Latitude: latitud del cliente\n",
    "- Drop_Longitude: longitud del cliente\n",
    "- Order_Date: fecha del pedido\n",
    "- Order_Time: hora del pedido \n",
    "- Pickup_Time: hora a la que el pedido fue recogido para su entrega\n",
    "- Weather: información sobre la climatología\n",
    "- Traffic: información sobre el tráfico\n",
    "- Vehicle: información sobre el vehículo\n",
    "- Area: información sobre el área de reparto\n",
    "- Category: categoría de los productos del pedido\n",
    "- Delivery_Time: tiempo de reparto (minutos)\n",
    "\n",
    "El objetivo de esta sección es abordar el análisis de este conjunto de datos y entrenar una red neuronal (Perceptrón Multicapa) para predecir el nivel de servicio. Aquí tienes algunos pasos que podrías seguir:\n",
    "\n",
    "1. **Análisis Exploratorio de Datos (EDA)**: Comienza explorando el conjunto de datos para comprender su estructura y distribución. Analiza la proporción de cada clase. Observa la distribución de las diferentes características y su relación con la clase objetivo \"class\".\n",
    "\n",
    "2. **Preprocesamiento de Datos**: Considera normalizar las características para que estén en la misma escala que las componentes principales.\n",
    "\n",
    "3. **Modelización**: Utiliza un perceptrón multicapa como herramienta de clasificación. Dado que el objetivo es identificar el nivel de servicio de la entrega, es vital centrarse en métricas como la precisión, la sensibilidad (recall), el valor F1 y el área bajo la curva ROC (AUC-ROC).\n",
    "\n",
    "4. **Evaluación**: Realiza una evaluación y análisis riguroso del rendimiento de tu modelo.\n",
    "\n",
    "Este enfoque integral te permitirá no solo construir un modelo efectivo sino también comprender mejor las características subyacentes del nivel de servicio en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAn2GmJcfHw_"
   },
   "source": [
    "<a id='ej20'></a>\n",
    "## 2.1. Carga de datos y procesamiento inicial (0.5 puntos)\n",
    "\n",
    "Lo primero que debes hacer es cargar el conjunto de datos y visualizar información relevante del mismo. Asegúrate de verificar lo siguiente:\n",
    "\n",
    "1. Confirma la cantidad total de filas y columnas en el DataFrame.\n",
    "2. Revisa el nombre de cada columna del DataFrame.\n",
    "3. Verifica el número de valores no nulos en cada columna.\n",
    "4. Identifica el tipo de datos de cada columna, que puede ser int, float, object, entre otros.\n",
    "5. Comprueba la cantidad de memoria utilizada por el DataFrame.\n",
    "\n",
    "Estos pasos te proporcionarán una comprensión inicial clara y detallada del conjunto de datos con el que estás trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = kagglehub.dataset_download(\"sujalsuthar/amazon-delivery-dataset\")\n",
    "dataset_path = os.path.join(root_path, \"amazon_delivery.csv\") \n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Cómo se puede observar, no disponemos del nivel de servicio en el conjunto de datos. Por ello, vamos a definir que todas las entregas realizadas en un máximo de dos horas han tenido un servicio Premium. Para ello, crea una nueva columna denominada \"Premium_Delivery\", que contenga el valor 1 si la entrega se ha realizado en un máximo de 120 minutos, y un 0 en caso contrario. Es importante asegurar que el tipo de la nueva columna sea entero.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Para simplificar el ejercicio y facilitar su comprensión, se deben eliminar las siguientes columnas: Order_ID, Order_Date, Order_Time y Pickup_Time\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "    \n",
    "A continuación, vamos a calcular la distancia harvesiana en kilómetros entre el almacén y el cliente. Para ello, debemos crear un nueva columna \"Distance\" y eliminar las cuatro columnas relacionadas con las coordenadas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwJqa9WIfHw_"
   },
   "source": [
    "<a id='ej21'></a>\n",
    "## 2.2. Análisis Exploratorio de Datos (EDA) (1.25 puntos)\n",
    "\n",
    "El Análisis Exploratorio de Datos (EDA, por sus siglas en inglés) en ciencia de datos es un enfoque inicial para comprender y resumir el contenido de un conjunto de datos. Este proceso implica varias técnicas y pasos:\n",
    "\n",
    "1. **Inspección de Datos**: Se comienza por revisar los datos brutos para identificar su estructura, tamaño y tipo (como numérico, categórico). Esto incluye detectar valores faltantes o inusuales.\n",
    "\n",
    "2. **Resumen Estadístico**: Se calculan estadísticas descriptivas como la media, mediana, rango, varianza y desviación estándar para obtener una idea general de las tendencias y patrones en los datos.\n",
    "\n",
    "3. **Visualización de Datos**: Se utilizan gráficos y diagramas (como histogramas, gráficos de caja, diagramas de dispersión) para visualizar distribuciones, relaciones entre variables y posibles anomalías. Esto ayuda a comprender mejor los datos y a identificar patrones o irregularidades.\n",
    "\n",
    "4. **Análisis de Relaciones y Correlaciones**: Se exploran las relaciones entre diferentes variables para entender cómo se influencian entre sí. Esto puede implicar el uso de matrices de correlación y gráficos de dispersión.\n",
    "\n",
    "5. **Identificación de Patrones y Anomalías**: Se buscan patrones consistentes o anomalías (como valores atípicos) que puedan sugerir tendencias o problemas en los datos.\n",
    "\n",
    "El EDA es una fase crítica en cualquier proyecto de ciencia de datos, ya que proporciona una comprensión profunda y una base sólida para posteriores análisis y modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIX-1GizfHw_",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "1. Calcula las frecuencias de la variable objetivo (`Premium_Delivery`) en tu conjunto de datos. \n",
    "2. Crea un gráfico de barras para visualizar estas frecuencias. Esto te ayudará a entender la proporción de entregas premium en comparación con las que no lo son.\n",
    "\n",
    "A continuación, analiza la distribución de las variables numéricas:\n",
    "\n",
    "1. Representa gráficamente el histograma de las variables, separando las observaciones según la clase a la que pertenecen (premium o no).\n",
    "2. Organiza todos los histogramas en un formato de 4 filas y 1 columna. Esto facilitará la comparación visual de las distribuciones para cada clase en cada variable.\n",
    "\n",
    "Por último, analiza la distribución de las variables categóricas de forma análoga a las variables numéricas, organizando todos los histogramas en un formato de 5 filas y 1 columna.\n",
    "\n",
    "Estos pasos te permitirán obtener una visión más clara de la estructura de tu conjunto de datos y cómo las diferentes variables pueden influir en la identificación de las entregas premium.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBJU0PIHfHw_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MFH2nvWfHw_"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rDxe75IfHw_"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "\n",
    "1. Evalúa la relación de las frecuencias de la variable `Premium_Delivery`. Reflexiona sobre cómo se distribuyen las transacciones entre premium y no no premium. ¿Es la distribución significativamente desigual? ¿Qué implica esto para el análisis y la modelización de los datos?\n",
    "2. Analiza la información proporcionada por los histogramas de las variables descriptoras. Observa si hay diferencias notables en las distribuciones de estas variables entre las clases. Pregúntate: ¿Hay variables que muestren patrones distintos para el nivel de servicio?\n",
    "3. Considera si hay otras formas de visualización que podrían ser útiles para entender mejor los datos. Por ejemplo, ¿serían útiles los diagramas de caja (boxplots) para visualizar la distribución de las variables en ambas clases? ¿Podría un mapa de calor de la matriz de correlación entre variables ayudarte a entender las relaciones entre ellas?\n",
    "\n",
    "Este análisis te ayudará a obtener una comprensión más profunda de la naturaleza de tus datos y a identificar posibles características que podrían ser importantes para detectar las entregas premium.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHW_jgcbfHw_",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oa3HbpukfHw_"
   },
   "source": [
    "<a id='ej22'></a>\n",
    "## 2.3. Preprocesamiento de Datos (1.25 puntos)\n",
    "\n",
    "El preprocesamiento de datos en ciencia de datos es un paso crucial que involucra la preparación y transformación de datos brutos en un formato adecuado para su posterior análisis y modelado. Este proceso incluye varias tareas esenciales:\n",
    "\n",
    "1. **Limpieza de Datos**: Se eliminan o corrigen datos erróneos, incompletos, inexactos o irrelevantes. Esto puede incluir tratar con valores faltantes, corregir errores de entrada y manejar outliers.\n",
    "\n",
    "2. **Normalización y Escalado**: Los datos se transforman para que estén en una escala común, sin distorsionar diferencias en los rangos de valores ni perder información. Por ejemplo, escalado min-max o estandarización.\n",
    "\n",
    "3. **Codificación de Variables Categóricas**: Las variables categóricas (como género o país) se convierten en formatos numéricos para que puedan ser procesadas por algoritmos de aprendizaje automático, utilizando técnicas como codificación one-hot o codificación de etiquetas.\n",
    "\n",
    "4. **División de Datos**: Los datos se dividen en conjuntos de entrenamiento, validación y prueba, permitiendo entrenar modelos, afinar hiperparámetros y evaluar el rendimiento del modelo de manera efectiva.\n",
    "\n",
    "5. **Manejo de Datos Desbalanceados**: En casos de conjuntos de datos desbalanceados, se aplican técnicas como sobremuestreo o submuestreo para asegurar que el modelo no esté sesgado hacia la clase más frecuente.\n",
    "\n",
    "6. **Ingeniería de Características**: Se crean nuevas variables (características) a partir de los datos existentes para mejorar la capacidad del modelo para aprender patrones y hacer predicciones.\n",
    "\n",
    "El preprocesamiento es esencial para mejorar la calidad de los datos y hacerlos más adecuados y efectivos para análisis y modelado en proyectos de ciencia de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong> elimina los atributos categóricos del conjunto de datos y en su lugar introduce la transformación de dichos atributos a tantas variables binarias como categorías tengan. Es importante que las nuevas columnas generadas sean de tipo entero. Recuerda que la codificación one-hot convierte las etiquetas categóricas en vectores binarios. En estos vectores, el valor de 1 se asigna a la posición correspondiente a la clase y el valor de 0 a todas las demás posiciones. Esto facilita que los modelos de aprendizaje automático procesen y entiendan las etiquetas categóricas.\n",
    "<hr>\n",
    "Sugerencia: utilizad la función \"get_dummies\" de \"pandas\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "En la primera sección del ejercicio pudimos observar que la columna Agent_Rating tenía un número reducido de valores nulos. En este ejercicio tenemos que imputar los valores nulos por el mínimo valor de la columna y verificar que ninguna columna adicional tiene valores nulos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu0SHBWbfHxA"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Ahora vamos a realizar la división del conjunto de datos, para ello sigue estos pasos:\n",
    "\n",
    "1. Separa los descriptores de la variable respuesta. Asigna los descriptores al conjunto `X` y la variable respuesta al conjunto `y`.\n",
    "2. Elimina del conjunto de descriptores la columna `Delivery_Time`, dado que fue la que utilizamos para calcular nuestra variable respuesta.\n",
    "3. Divide el _dataset_ en dos subconjuntos: uno para entrenamiento (_train_) y otro para pruebas (_test_). Asigna el 80% de los datos al conjunto de entrenamiento (`X_train`, `y_train`) y el 20% al conjunto de pruebas (`X_test`, `y_test`). Utiliza la función `train_test_split` de la biblioteca `model_selection` de `sklearn`. Asegúrate de usar `random_state = 24` y haz una división estratificada para mantener la misma proporción de clases en ambos conjuntos.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj__HV48fHxA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0e2C3NzfHxA"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKCW1Rh1fHxA"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "1. Normaliza los descriptores utilizando el `StandardScaler` de `sklearn`. Esto estandarizará las características restando la media y dividiendo por la desviación estándar.\n",
    "2. Muestra las dimensiones del conjunto de descriptores original, del conjunto de entrenamiento y del conjunto de prueba. Esto te permitirá ver cómo se han dividido los datos.\n",
    "\n",
    "<strong>Nota:</strong> Ajusta el `StandardScaler` únicamente con los descriptores de entrenamiento para evitar la fuga de información o 'data leakage'. La fuga de información ocurre cuando se utiliza información del conjunto de prueba o validación en el proceso de ajuste del modelo. Es decir, si ajustas el modelo de escalado con todo el conjunto de datos, estarías utilizando información del conjunto de prueba o validación en el ajuste, lo que podría dar la impresión de que el modelo es más preciso de lo que realmente es. Por lo tanto, asegúrate de ajustar el `StandardScaler` solo con los datos de entrenamiento y luego aplicarlo a los conjuntos de entrenamiento y prueba.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrgdxoZZfHxA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxe-T3-mfHxA"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Convierte los conjunto de datos de entrenamiento y test en tensores, utilizando el método `tensor` de la librería `PyTorch`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cpg-RIGpfHxA"
   },
   "source": [
    "<a id='ej23'></a>\n",
    "## 2.4. Modelización (2 puntos)\n",
    "\n",
    "El MLP (Perceptrón Multicapa) es, sin duda, una poderosa herramienta en el campo del aprendizaje automático y la inteligencia artificial. Puede manejar tareas de clasificación y regresión, lo que lo hace versátil para una variedad de problemas. Su capacidad para modelar relaciones no lineales complejas lo convierte en una elección popular cuando los datos no siguen patrones lineales simples.\n",
    "\n",
    "Aquí hay algunos puntos clave sobre el MLP:\n",
    "\n",
    "- **Capas y Neuronas**: El MLP consta de múltiples capas de neuronas, que incluyen una capa de entrada, una o más capas ocultas y una capa de salida. Cada neurona en una capa está conectada a todas las neuronas en la capa siguiente.\n",
    "\n",
    "- **Funciones de Activación**: Para introducir no linealidad en el modelo, se utilizan funciones de activación en las neuronas, como la función sigmoide, ReLU (Rectified Linear Unit) o tangente hiperbólica. Estas funciones permiten al MLP capturar patrones complejos en los datos.\n",
    "\n",
    "- **Aprendizaje Supervisado**: El entrenamiento del MLP implica ajustar los pesos de las conexiones entre neuronas para minimizar la diferencia entre las salidas producidas por la red y las salidas deseadas. Esto se hace utilizando algoritmos de aprendizaje supervisado, como el descenso del gradiente.\n",
    "\n",
    "- **Ajuste de Hiperparámetros**: Al igual que otros modelos de aprendizaje automático, el MLP tiene hiperparámetros importantes, como el número de capas ocultas, el número de neuronas en cada capa, la función de activación y la tasa de aprendizaje. A menudo, es necesario ajustar estos hiperparámetros para obtener un buen rendimiento en una tarea específica.\n",
    "\n",
    "- **Generalización**: Uno de los desafíos en el entrenamiento de MLP es evitar el sobreajuste (overfitting), donde el modelo se adapta demasiado a los datos de entrenamiento y no generaliza bien a datos nuevos. La regularización y la validación cruzada son técnicas comunes para abordar este problema.\n",
    "\n",
    "En este contexto, el MLP puede ser una excelente opción para modelar patrones complejos que indiquen cuando una entrega será premium. Sin embargo, es importante ajustar y evaluar cuidadosamente el modelo para garantizar que funcione de manera efectiva en esta tarea crítica.\n",
    "\n",
    "Crear y entrenar un MLP con varias capas ocultas con función de activación ReLU es una excelente elección. La función de activación ReLU (Rectified Linear Unit) es comúnmente utilizada en capas ocultas de redes neuronales debido a su capacidad para introducir no linealidad en el modelo, lo que le permite aprender patrones complejos en los datos.\n",
    "\n",
    "Por otra parte, el enfoque de apilar capas lineales utilizando la clase `Linear` de PyTorch es una forma eficaz y sencilla de construir modelos de redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, n_epochs, batch_size):\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    best_acc = - np.inf\n",
    "    best_weights = None\n",
    "\n",
    "    train_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_loss_hist = []\n",
    "    val_acc_hist = []\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        model.train()\n",
    "        \n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # compute and store metrics\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                epoch_loss.append(float(loss))\n",
    "                epoch_acc.append(float(acc))\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "\n",
    "        # Evaluating the model at the end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        ce = float(loss_fn(y_pred, y_val))\n",
    "        acc = float((y_pred.round() == y_val).float().mean())\n",
    "\n",
    "        train_loss_hist.append(np.mean(epoch_loss))\n",
    "        train_acc_hist.append(np.mean(epoch_acc))\n",
    "        val_loss_hist.append(ce)\n",
    "        val_acc_hist.append(acc)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "        # print(f\"Epoch {epoch} validation: Cross-entropy={ce:.2f}, Accuracy={acc*100:.1f}%\")\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc, train_loss_hist, val_loss_hist, train_acc_hist, val_acc_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsMGXeDEfHxA"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "    \n",
    "La modealización del MLP la vamos a realizar con la librería `PyTorch`. Para ello:\n",
    "\n",
    "1. Comienza creando el modelo `BinaryServiceLevel`, para lo cual es necesario crear una clase que herede de `nn.Module`.\n",
    "2. En el constructor (`__init__`), declara las siguientes capas:\n",
    "    - Una capa lineal `nn.Linear` de entrada con un tamaño de salida de 19, y una función de activación ReLu `nn.ReLu`.\n",
    "    - Una capa lineal `nn.Linear` con un tamaño de salida de 19, y una función de activación ReLu `nn.ReLu`.\n",
    "    - Una capa lineal `nn.Linear` de salida con una función de activación Sigmoid `nn.Sigmoid`.\n",
    "3. Después, en el método `forward` enlaza las diferentes capas y sus respectivas funciones de activación en el orden definido en el punto anterior. \n",
    "4. No olvides mostrar el número de parámetros utilizando el método `.parameters()` del modelo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27hz7RN_fHxA"
   },
   "outputs": [],
   "source": [
    "class BinaryServiceLevelBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "  \n",
    "1. Ahora, es hora de entrenar y validar el modelo aplicando validación cruzada utilizando `StratifiedKFold` sobre el conjunto de entrenamiento, con un valor de k = 5 y shuffle = True.\n",
    "2. En cada split, el modelo se debe entrenar utilizando la función `train_model`. Asegúrate de entrenar con los datos de entrenamiento y validación de cada split, establece el número de épocas en 15 y el tamaño del lote en 32.\n",
    "3. En cada iteración se deben calcular las siguientes métricas:\n",
    "    - Calcula la exactitud (accuracy) para medir la exactitud de las predicciones.\n",
    "    - Calcula el valor F1, que es una medida que combina exactitud y sensibilidad.\n",
    "    - Calcula el área bajo la curva ROC (AUC-ROC) para evaluar el rendimiento del modelo en la clasificación binaria.\n",
    "4. Por último, se debe mostrar la media de cada de las métricas calculadas en el punto anterior.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "    \n",
    "1. Realiza un análisis de los resultados y decide si consideras que este modelo es aceptable.\n",
    "2. Evalúa cuál de las medidas de rendimiento utilizadas es la más apropiada.\n",
    "3. Examina la distribución de las clases y plantea una estrategia, si es necesario, para asegurar la confiabilidad del estudio realizado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "  \n",
    "Ahora, es hora de analizar la fase de entrenamiento, para ello:\n",
    " \n",
    "1. Divide el dataset de entrenamiento en dos subconjuntos a su vez: uno para entrenamiento (train) y otro para validación (val), asignando el 80% de los datos al conjunto de entrenamiento.\n",
    "2. Entrena el modelo con la función `model_train`, guardando todos los parámetros que devuelve.\n",
    "3. Crea gráficos que muestren la pérdida (`loss`) tanto en el entrenamiento como en la validación a lo largo de las épocas.\n",
    "4. Por último, genera gráficos que representen la exactitud (`accuracy`) en el entrenamiento y la validación a lo largo de las épocas.\n",
    "\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "\n",
    "¿Que conclusiones puedes obtener de las gráficas tanto de la pérdida (`loss`) como de la exactitud (`accuracy`) en el entrenamiento y la validación a lo largo de las épocas?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "  \n",
    "Es hora evaluar el rendimiento del modelo en el conjunto de test. Para ello:\n",
    "   \n",
    "1. Realiza la predicción sobre el conjunto de test.\n",
    "2. Calcula las métricas de los apartados anteriores: accuracy, f1 score y curva roc.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementación:</strong>\n",
    "\n",
    "Para analizar el modelo entrenado, nos vamos a apoyar en los [Shapley values](https://en.wikipedia.org/wiki/Shapley_value), los cuales nos introducen a la explicación de modelos de aprendizaje automático. El objetivo es explicar la predicción de un modelo calculando la contribución de cada característica a la predicción. La explicación técnica del concepto de SHAP es el cálculo de los valores de Shapley a partir de la teoría de juegos. En pocas palabras, los valores de Shapley son un método para mostrar el impacto relativo de cada característica (o variable) que estamos midiendo en el resultado final del modelo de aprendizaje automático comparando el efecto relativo de las entradas con la media.\n",
    "\n",
    "Para calcular los _shap values_:\n",
    "\n",
    "1. Selecciona una muestra de 10000 registros del conjunto de entrenamiento.\n",
    "2. Inicializa el 'explainer' `shap.DeepExplainer` con el modelo entrenado y la muestra anterior.\n",
    "3. Selecciona una muestra de 400 registros del conjunto de entrenamiento.\n",
    "4. Calcula los _shap_ values utilizando la muestra anterior\n",
    "5. Define y muestra un DataFrame con tres columnas\n",
    "   - Media aritmética del valor absoluto de los valores\n",
    "   - Desviación típica del valor absoluto de los valores\n",
    "   - Nombre del atributo descriptivo\n",
    "6. Muestra la representación gráfica de los valores utilizando la librería shap.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Solución:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "    \n",
    "Relaciona la interpretación de los _shap values_ con el análisis exploratorio de los datos realizado en el ejercicio 2.2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Análisis:</strong>\n",
    "    \n",
    "Imagina que calculamos los _shap values_ para cada split en el ejercicio en el que entrenamos el modelo utilizando validación cruzada. \n",
    "- ¿Los análisis de los diferentes modelos deberían ser similares? ¿Por qué o por qué no?\n",
    "- ¿Qué indicaría si el análisis de cada modelo varía mucho de uno a otro?\n",
    "- ¿Qué usos adicionales les podemos dar a los _shap values_?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
